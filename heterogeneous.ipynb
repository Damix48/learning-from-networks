{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GATConv, Linear, to_hetero\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_edges(edges, mapping_1, mapping_2):\n",
    "    edges_ = torch.empty((2, len(edges)), dtype=torch.long)\n",
    "    for i, row in edges.iterrows():\n",
    "        try:\n",
    "            edges_[0][i] = mapping_1[row.iloc[0]]\n",
    "            edges_[1][i] = mapping_2[row.iloc[1]]\n",
    "        except:\n",
    "            edges_[0][i] = -1\n",
    "            edges_[1][i] = -1\n",
    "\n",
    "    # Remove invalid edges\n",
    "    edges_ = edges_[:, edges_[0] != -1]\n",
    "\n",
    "    return edges_\n",
    "\n",
    "\n",
    "def expand_columns(df, cols):\n",
    "    num_cols_ = {}\n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(ast.literal_eval)\n",
    "\n",
    "        num_cols = len(df[col][0])\n",
    "        num_cols_[col] = num_cols\n",
    "\n",
    "        cols = pd.DataFrame(\n",
    "            df[col].tolist(), columns=[f\"{col}_{i}\" for i in range(num_cols)]\n",
    "        )\n",
    "\n",
    "        df = pd.concat([df, cols], axis=1)\n",
    "\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "    return df, num_cols_\n",
    "\n",
    "\n",
    "def create_hetero_data(path):\n",
    "    movies = pd.read_csv(f\"{path}/movies.csv\")\n",
    "    actors = pd.read_csv(f\"{path}/actors.csv\")\n",
    "    directors = pd.read_csv(f\"{path}/directors.csv\")\n",
    "    writers = pd.read_csv(f\"{path}/writers.csv\")\n",
    "\n",
    "    movie_actor_ = pd.read_csv(f\"{path}/movie_actor.csv\")\n",
    "    movie_director_ = pd.read_csv(f\"{path}/movie_director.csv\")\n",
    "    movie_writer_ = pd.read_csv(f\"{path}/movie_writer.csv\")\n",
    "\n",
    "    actor_actor_ = pd.read_csv(f\"{path}/actor_actor.csv\")\n",
    "    actor_director_ = pd.read_csv(f\"{path}/actor_director.csv\")\n",
    "    actor_writer_ = pd.read_csv(f\"{path}/actor_writer.csv\")\n",
    "\n",
    "    director_director_ = pd.read_csv(f\"{path}/director_director.csv\")\n",
    "    director_writer_ = pd.read_csv(f\"{path}/director_writer.csv\")\n",
    "\n",
    "    writer_writer_ = pd.read_csv(f\"{path}/writer_writer.csv\")\n",
    "\n",
    "    movies, num_cols = expand_columns(movies, [\"genres\"])\n",
    "\n",
    "    movies[\"release_date\"] = pd.to_datetime(movies[\"release_date\"])\n",
    "    first_date = movies[\"release_date\"].min()\n",
    "    movies[\"release_date\"] = (movies[\"release_date\"] - first_date).dt.days\n",
    "\n",
    "    movies_id = movies[\"id\"].tolist()\n",
    "    actors_id = actors[\"id\"].tolist()\n",
    "    directors_id = directors[\"id\"].tolist()\n",
    "    writers_id = writers[\"id\"].tolist()\n",
    "\n",
    "    movies_map = dict(zip(movies_id, range(len(movies_id))))\n",
    "    actors_map = dict(zip(actors_id, range(len(actors_id))))\n",
    "    directors_map = dict(zip(directors_id, range(len(directors_id))))\n",
    "    writers_map = dict(zip(writers_id, range(len(writers_id))))\n",
    "\n",
    "    data = HeteroData()\n",
    "\n",
    "    data[\"movie\"].ids = torch.tensor(movies_id, dtype=torch.long)\n",
    "    data[\"movie\"].x = torch.tensor(\n",
    "        movies[\n",
    "            [\n",
    "                \"release_date\",\n",
    "                \"budget\",\n",
    "                \"revenue\",\n",
    "                \"vote_average\",\n",
    "                \"vote_count\",\n",
    "                \"popularity\",\n",
    "            ]\n",
    "            + [f\"genres_{i}\" for i in range(num_cols[\"genres\"])]\n",
    "        ].values.tolist(),\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "\n",
    "    # normalize\n",
    "    data[\"movie\"].x[:, :6] = F.normalize(data[\"movie\"].x[:, :6], dim=0)\n",
    "    data[\"movie\"].x[:, 6:] = F.normalize(data[\"movie\"].x[:, 6:], dim=1)\n",
    "\n",
    "    data[\"actor\"].ids = torch.tensor(actors_id, dtype=torch.long)\n",
    "    data[\"actor\"].x = torch.ones((len(actors_id), 1), dtype=torch.float)\n",
    "    data[\"actor\"].y = torch.tensor(\n",
    "        actors[[\"popularity\"]].values.tolist(), dtype=torch.float\n",
    "    )\n",
    "\n",
    "    data[\"actor\"].y = torch.log(data[\"actor\"].y)\n",
    "\n",
    "    data[\"director\"].ids = torch.tensor(directors_id, dtype=torch.long)\n",
    "    data[\"director\"].x = torch.tensor(\n",
    "        directors[[\"popularity\"]].values.tolist(), dtype=torch.float\n",
    "    )\n",
    "\n",
    "    data[\"writer\"].ids = torch.tensor(writers_id, dtype=torch.long)\n",
    "    data[\"writer\"].x = torch.tensor(\n",
    "        writers[[\"popularity\"]].values.tolist(), dtype=torch.float\n",
    "    )\n",
    "\n",
    "    data[\"movie\", \"played_by\", \"actor\"].edge_index = convert_edges(\n",
    "        movie_actor_, movies_map, actors_map\n",
    "    )\n",
    "\n",
    "    data[\"movie\", \"directed_by\", \"director\"].edge_index = convert_edges(\n",
    "        movie_director_, movies_map, directors_map\n",
    "    )\n",
    "    data[\"movie\", \"written_by\", \"writer\"].edge_index = convert_edges(\n",
    "        movie_writer_, movies_map, writers_map\n",
    "    )\n",
    "\n",
    "    data[\"actor\", \"same_page\", \"actor\"].edge_index = convert_edges(\n",
    "        actor_actor_, actors_map, actors_map\n",
    "    )\n",
    "    data[\"actor\", \"same_page\", \"director\"].edge_index = convert_edges(\n",
    "        actor_director_, actors_map, directors_map\n",
    "    )\n",
    "    data[\"actor\", \"same_page\", \"writer\"].edge_index = convert_edges(\n",
    "        actor_writer_, actors_map, writers_map\n",
    "    )\n",
    "\n",
    "    data[\"director\", \"same_page\", \"director\"].edge_index = convert_edges(\n",
    "        director_director_, directors_map, directors_map\n",
    "    )\n",
    "    data[\"director\", \"same_page\", \"writer\"].edge_index = convert_edges(\n",
    "        director_writer_, directors_map, writers_map\n",
    "    )\n",
    "\n",
    "    data[\"writer\", \"same_page\", \"writer\"].edge_index = convert_edges(\n",
    "        writer_writer_, writers_map, writers_map\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_hetero_data(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  movie={\n",
      "    ids=[13337],\n",
      "    x=[13337, 25],\n",
      "  },\n",
      "  actor={\n",
      "    ids=[54506],\n",
      "    x=[54506, 1],\n",
      "    y=[54506, 1],\n",
      "  },\n",
      "  director={\n",
      "    ids=[9291],\n",
      "    x=[9291, 1],\n",
      "  },\n",
      "  writer={\n",
      "    ids=[11412],\n",
      "    x=[11412, 1],\n",
      "  },\n",
      "  (movie, played_by, actor)={ edge_index=[2, 96751] },\n",
      "  (movie, directed_by, director)={ edge_index=[2, 12357] },\n",
      "  (movie, written_by, writer)={ edge_index=[2, 14432] },\n",
      "  (actor, same_page, actor)={ edge_index=[2, 119604] },\n",
      "  (actor, same_page, director)={ edge_index=[2, 33168] },\n",
      "  (actor, same_page, writer)={ edge_index=[2, 30598] },\n",
      "  (director, same_page, director)={ edge_index=[2, 7394] },\n",
      "  (director, same_page, writer)={ edge_index=[2, 8201] },\n",
      "  (writer, same_page, writer)={ edge_index=[2, 7395] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "data = T.ToUndirected()(data)\n",
    "data = T.AddSelfLoops()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/admin/networks/wandb/run-20240126_161406-xkuwmfqz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/damix48/movie-actor/runs/xkuwmfqz' target=\"_blank\">daily-breeze-15</a></strong> to <a href='https://wandb.ai/damix48/movie-actor' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/damix48/movie-actor' target=\"_blank\">https://wandb.ai/damix48/movie-actor</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/damix48/movie-actor/runs/xkuwmfqz' target=\"_blank\">https://wandb.ai/damix48/movie-actor/runs/xkuwmfqz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 214.8935\n",
      "Epoch: 002, Loss: 212.0808\n",
      "Epoch: 003, Loss: 209.3150\n",
      "Epoch: 004, Loss: 206.3878\n",
      "Epoch: 005, Loss: 203.0734\n",
      "Epoch: 006, Loss: 199.3001\n",
      "Epoch: 007, Loss: 195.0981\n",
      "Epoch: 008, Loss: 190.4244\n",
      "Epoch: 009, Loss: 185.1948\n",
      "Epoch: 010, Loss: 179.3391\n",
      "Epoch: 010, loss: 181.8727, score: -0.2460\n",
      "Epoch: 011, Loss: 172.7906\n",
      "Epoch: 012, Loss: 165.4415\n",
      "Epoch: 013, Loss: 157.2209\n",
      "Epoch: 014, Loss: 148.3226\n",
      "Epoch: 015, Loss: 138.3110\n",
      "Epoch: 016, Loss: 128.3773\n",
      "Epoch: 017, Loss: 118.9351\n",
      "Epoch: 018, Loss: 111.4832\n",
      "Epoch: 019, Loss: 107.4496\n",
      "Epoch: 020, Loss: 107.0978\n",
      "Epoch: 020, loss: 112.4844, score: 0.2294\n",
      "Epoch: 021, Loss: 108.5304\n",
      "Epoch: 022, Loss: 110.7630\n",
      "Epoch: 023, Loss: 112.3195\n",
      "Epoch: 024, Loss: 112.1562\n",
      "Epoch: 025, Loss: 110.3277\n",
      "Epoch: 026, Loss: 107.4334\n",
      "Epoch: 027, Loss: 104.2009\n",
      "Epoch: 028, Loss: 101.2034\n",
      "Epoch: 029, Loss: 98.7648\n",
      "Epoch: 030, Loss: 96.9979\n",
      "Epoch: 030, loss: 100.3832, score: 0.3123\n",
      "Epoch: 031, Loss: 95.8446\n",
      "Epoch: 032, Loss: 95.1447\n",
      "Epoch: 033, Loss: 94.7064\n",
      "Epoch: 034, Loss: 94.3586\n",
      "Epoch: 035, Loss: 93.9759\n",
      "Epoch: 036, Loss: 93.4941\n",
      "Epoch: 037, Loss: 92.9098\n",
      "Epoch: 038, Loss: 92.2703\n",
      "Epoch: 039, Loss: 91.6603\n",
      "Epoch: 040, Loss: 91.1834\n",
      "Epoch: 040, loss: 95.1734, score: 0.3480\n",
      "Epoch: 041, Loss: 90.9309\n",
      "Epoch: 042, Loss: 90.9360\n",
      "Epoch: 043, Loss: 91.1276\n",
      "Epoch: 044, Loss: 91.3505\n",
      "Epoch: 045, Loss: 91.4440\n",
      "Epoch: 046, Loss: 91.3265\n",
      "Epoch: 047, Loss: 91.0326\n",
      "Epoch: 048, Loss: 90.6663\n",
      "Epoch: 049, Loss: 90.3327\n",
      "Epoch: 050, Loss: 90.0942\n",
      "Epoch: 050, loss: 94.2790, score: 0.3541\n",
      "Epoch: 051, Loss: 89.9570\n",
      "Epoch: 052, Loss: 89.8849\n",
      "Epoch: 053, Loss: 89.8255\n",
      "Epoch: 054, Loss: 89.7397\n",
      "Epoch: 055, Loss: 89.6094\n",
      "Epoch: 056, Loss: 89.4397\n",
      "Epoch: 057, Loss: 89.2562\n",
      "Epoch: 058, Loss: 89.0896\n",
      "Epoch: 059, Loss: 88.9649\n",
      "Epoch: 060, Loss: 88.8882\n",
      "Epoch: 060, loss: 93.1015, score: 0.3622\n",
      "Epoch: 061, Loss: 88.8451\n",
      "Epoch: 062, Loss: 88.8082\n",
      "Epoch: 063, Loss: 88.7517\n",
      "Epoch: 064, Loss: 88.6656\n",
      "Epoch: 065, Loss: 88.5573\n",
      "Epoch: 066, Loss: 88.4445\n",
      "Epoch: 067, Loss: 88.3434\n",
      "Epoch: 068, Loss: 88.2588\n",
      "Epoch: 069, Loss: 88.1875\n",
      "Epoch: 070, Loss: 88.1207\n",
      "Epoch: 070, loss: 92.5825, score: 0.3657\n",
      "Epoch: 071, Loss: 88.0494\n",
      "Epoch: 072, Loss: 87.9690\n",
      "Epoch: 073, Loss: 87.8848\n",
      "Epoch: 074, Loss: 87.8023\n",
      "Epoch: 075, Loss: 87.7299\n",
      "Epoch: 076, Loss: 87.6681\n",
      "Epoch: 077, Loss: 87.6140\n",
      "Epoch: 078, Loss: 87.5619\n",
      "Epoch: 079, Loss: 87.5065\n",
      "Epoch: 080, Loss: 87.4479\n",
      "Epoch: 080, loss: 91.8391, score: 0.3708\n",
      "Epoch: 081, Loss: 87.3862\n",
      "Epoch: 082, Loss: 87.3278\n",
      "Epoch: 083, Loss: 87.2721\n",
      "Epoch: 084, Loss: 87.2202\n",
      "Epoch: 085, Loss: 87.1694\n",
      "Epoch: 086, Loss: 87.1142\n",
      "Epoch: 087, Loss: 87.0529\n",
      "Epoch: 088, Loss: 86.9855\n",
      "Epoch: 089, Loss: 86.9146\n",
      "Epoch: 090, Loss: 86.8423\n",
      "Epoch: 090, loss: 91.4008, score: 0.3738\n",
      "Epoch: 091, Loss: 86.7695\n",
      "Epoch: 092, Loss: 86.6965\n",
      "Epoch: 093, Loss: 86.6254\n",
      "Epoch: 094, Loss: 86.5544\n",
      "Epoch: 095, Loss: 86.4814\n",
      "Epoch: 096, Loss: 86.4081\n",
      "Epoch: 097, Loss: 86.3351\n",
      "Epoch: 098, Loss: 86.2617\n",
      "Epoch: 099, Loss: 86.1849\n",
      "Epoch: 100, Loss: 86.1039\n",
      "Epoch: 100, loss: 90.7676, score: 0.3782\n",
      "Epoch: 101, Loss: 86.0212\n",
      "Epoch: 102, Loss: 85.9388\n",
      "Epoch: 103, Loss: 85.8539\n",
      "Epoch: 104, Loss: 85.7656\n",
      "Epoch: 105, Loss: 85.6674\n",
      "Epoch: 106, Loss: 85.5678\n",
      "Epoch: 107, Loss: 85.4644\n",
      "Epoch: 108, Loss: 85.3542\n",
      "Epoch: 109, Loss: 85.2443\n",
      "Epoch: 110, Loss: 85.1421\n",
      "Epoch: 110, loss: 90.0474, score: 0.3831\n",
      "Epoch: 111, Loss: 85.0447\n",
      "Epoch: 112, Loss: 84.9452\n",
      "Epoch: 113, Loss: 84.8452\n",
      "Epoch: 114, Loss: 84.7470\n",
      "Epoch: 115, Loss: 84.6544\n",
      "Epoch: 116, Loss: 84.5666\n",
      "Epoch: 117, Loss: 84.4793\n",
      "Epoch: 118, Loss: 84.3920\n",
      "Epoch: 119, Loss: 84.3063\n",
      "Epoch: 120, Loss: 84.2243\n",
      "Epoch: 120, loss: 89.2675, score: 0.3884\n",
      "Epoch: 121, Loss: 84.1456\n",
      "Epoch: 122, Loss: 84.0644\n",
      "Epoch: 123, Loss: 83.9823\n",
      "Epoch: 124, Loss: 83.9005\n",
      "Epoch: 125, Loss: 83.8221\n",
      "Epoch: 126, Loss: 83.7433\n",
      "Epoch: 127, Loss: 83.6652\n",
      "Epoch: 128, Loss: 83.5997\n",
      "Epoch: 129, Loss: 83.5366\n",
      "Epoch: 130, Loss: 83.4759\n",
      "Epoch: 130, loss: 88.6505, score: 0.3927\n",
      "Epoch: 131, Loss: 83.4147\n",
      "Epoch: 132, Loss: 83.3529\n",
      "Epoch: 133, Loss: 83.2919\n",
      "Epoch: 134, Loss: 83.2326\n",
      "Epoch: 135, Loss: 83.1742\n",
      "Epoch: 136, Loss: 83.1166\n",
      "Epoch: 137, Loss: 83.0604\n",
      "Epoch: 138, Loss: 83.0058\n",
      "Epoch: 139, Loss: 82.9519\n",
      "Epoch: 140, Loss: 82.8986\n",
      "Epoch: 140, loss: 88.2596, score: 0.3954\n",
      "Epoch: 141, Loss: 82.8457\n",
      "Epoch: 142, Loss: 82.7930\n",
      "Epoch: 143, Loss: 82.7411\n",
      "Epoch: 144, Loss: 82.6899\n",
      "Epoch: 145, Loss: 82.6402\n",
      "Epoch: 146, Loss: 82.5893\n",
      "Epoch: 147, Loss: 82.5372\n",
      "Epoch: 148, Loss: 82.4839\n",
      "Epoch: 149, Loss: 82.4302\n",
      "Epoch: 150, Loss: 82.3774\n",
      "Epoch: 150, loss: 87.7720, score: 0.3987\n",
      "Epoch: 151, Loss: 82.3258\n",
      "Epoch: 152, Loss: 82.2745\n",
      "Epoch: 153, Loss: 82.2222\n",
      "Epoch: 154, Loss: 82.1704\n",
      "Epoch: 155, Loss: 82.1194\n",
      "Epoch: 156, Loss: 82.0679\n",
      "Epoch: 157, Loss: 82.0155\n",
      "Epoch: 158, Loss: 81.9629\n",
      "Epoch: 159, Loss: 81.9096\n",
      "Epoch: 160, Loss: 81.8557\n",
      "Epoch: 160, loss: 87.3384, score: 0.4017\n",
      "Epoch: 161, Loss: 81.8030\n",
      "Epoch: 162, Loss: 81.7515\n",
      "Epoch: 163, Loss: 81.7000\n",
      "Epoch: 164, Loss: 81.6478\n",
      "Epoch: 165, Loss: 81.5944\n",
      "Epoch: 166, Loss: 81.5409\n",
      "Epoch: 167, Loss: 81.4856\n",
      "Epoch: 168, Loss: 81.4243\n",
      "Epoch: 169, Loss: 81.3618\n",
      "Epoch: 170, Loss: 81.3658\n",
      "Epoch: 170, loss: 86.8517, score: 0.4050\n",
      "Epoch: 171, Loss: 81.2365\n",
      "Epoch: 172, Loss: 81.2995\n",
      "Epoch: 173, Loss: 81.1339\n",
      "Epoch: 174, Loss: 81.1624\n",
      "Epoch: 175, Loss: 81.0316\n",
      "Epoch: 176, Loss: 81.0521\n",
      "Epoch: 177, Loss: 80.9355\n",
      "Epoch: 178, Loss: 80.9375\n",
      "Epoch: 179, Loss: 80.8495\n",
      "Epoch: 180, Loss: 80.8240\n",
      "Epoch: 180, loss: 86.5997, score: 0.4067\n",
      "Epoch: 181, Loss: 80.7660\n",
      "Epoch: 182, Loss: 80.7142\n",
      "Epoch: 183, Loss: 80.6807\n",
      "Epoch: 184, Loss: 80.6177\n",
      "Epoch: 185, Loss: 80.5954\n",
      "Epoch: 186, Loss: 80.5380\n",
      "Epoch: 187, Loss: 80.5096\n",
      "Epoch: 188, Loss: 80.4553\n",
      "Epoch: 189, Loss: 80.4221\n",
      "Epoch: 190, Loss: 80.3840\n",
      "Epoch: 190, loss: 86.2163, score: 0.4093\n",
      "Epoch: 191, Loss: 80.3351\n",
      "Epoch: 192, Loss: 80.3139\n",
      "Epoch: 193, Loss: 80.2559\n",
      "Epoch: 194, Loss: 80.2366\n",
      "Epoch: 195, Loss: 80.1852\n",
      "Epoch: 196, Loss: 80.1508\n",
      "Epoch: 197, Loss: 80.1170\n",
      "Epoch: 198, Loss: 80.0660\n",
      "Epoch: 199, Loss: 80.0421\n",
      "Epoch: 200, Loss: 79.9962\n",
      "Epoch: 200, loss: 85.9748, score: 0.4110\n",
      "Epoch: 201, Loss: 79.9504\n",
      "Epoch: 202, Loss: 79.9202\n",
      "Epoch: 203, Loss: 79.8809\n",
      "Epoch: 204, Loss: 79.8361\n",
      "Epoch: 205, Loss: 79.7966\n",
      "Epoch: 206, Loss: 79.7610\n",
      "Epoch: 207, Loss: 79.7253\n",
      "Epoch: 208, Loss: 79.6902\n",
      "Epoch: 209, Loss: 79.6593\n",
      "Epoch: 210, Loss: 79.6274\n",
      "Epoch: 210, loss: 85.8475, score: 0.4119\n",
      "Epoch: 211, Loss: 79.5914\n",
      "Epoch: 212, Loss: 79.5577\n",
      "Epoch: 213, Loss: 79.5139\n",
      "Epoch: 214, Loss: 79.4826\n",
      "Epoch: 215, Loss: 79.4547\n",
      "Epoch: 216, Loss: 79.4309\n",
      "Epoch: 217, Loss: 79.4062\n",
      "Epoch: 218, Loss: 79.3586\n",
      "Epoch: 219, Loss: 79.3069\n",
      "Epoch: 220, Loss: 79.2572\n",
      "Epoch: 220, loss: 85.6415, score: 0.4133\n",
      "Epoch: 221, Loss: 79.2201\n",
      "Epoch: 222, Loss: 79.1952\n",
      "Epoch: 223, Loss: 79.1780\n",
      "Epoch: 224, Loss: 79.1565\n",
      "Epoch: 225, Loss: 79.1129\n",
      "Epoch: 226, Loss: 79.0690\n",
      "Epoch: 227, Loss: 79.0359\n",
      "Epoch: 228, Loss: 79.0168\n",
      "Epoch: 229, Loss: 79.0029\n",
      "Epoch: 230, Loss: 78.9715\n",
      "Epoch: 230, loss: 85.5181, score: 0.4141\n",
      "Epoch: 231, Loss: 78.9341\n",
      "Epoch: 232, Loss: 78.8948\n",
      "Epoch: 233, Loss: 78.8596\n",
      "Epoch: 234, Loss: 78.8267\n",
      "Epoch: 235, Loss: 78.8022\n",
      "Epoch: 236, Loss: 78.7819\n",
      "Epoch: 237, Loss: 78.7752\n",
      "Epoch: 238, Loss: 78.7826\n",
      "Epoch: 239, Loss: 78.7500\n",
      "Epoch: 240, Loss: 78.7059\n",
      "Epoch: 240, loss: 85.3804, score: 0.4151\n",
      "Epoch: 241, Loss: 78.6324\n",
      "Epoch: 242, Loss: 78.6142\n",
      "Epoch: 243, Loss: 78.6157\n",
      "Epoch: 244, Loss: 78.5717\n",
      "Epoch: 245, Loss: 78.5193\n",
      "Epoch: 246, Loss: 78.4978\n",
      "Epoch: 247, Loss: 78.5024\n",
      "Epoch: 248, Loss: 78.4889\n",
      "Epoch: 249, Loss: 78.4557\n",
      "Epoch: 250, Loss: 78.3946\n",
      "Epoch: 250, loss: 85.2808, score: 0.4158\n",
      "Epoch: 251, Loss: 78.3613\n",
      "Epoch: 252, Loss: 78.3509\n",
      "Epoch: 253, Loss: 78.3281\n",
      "Epoch: 254, Loss: 78.2906\n",
      "Epoch: 255, Loss: 78.2541\n",
      "Epoch: 256, Loss: 78.2365\n",
      "Epoch: 257, Loss: 78.2290\n",
      "Epoch: 258, Loss: 78.2162\n",
      "Epoch: 259, Loss: 78.2140\n",
      "Epoch: 260, Loss: 78.1702\n",
      "Epoch: 260, loss: 85.1552, score: 0.4166\n",
      "Epoch: 261, Loss: 78.1267\n",
      "Epoch: 262, Loss: 78.0773\n",
      "Epoch: 263, Loss: 78.0693\n",
      "Epoch: 264, Loss: 78.0537\n",
      "Epoch: 265, Loss: 78.0115\n",
      "Epoch: 266, Loss: 77.9778\n",
      "Epoch: 267, Loss: 77.9591\n",
      "Epoch: 268, Loss: 77.9474\n",
      "Epoch: 269, Loss: 77.9323\n",
      "Epoch: 270, Loss: 77.9007\n",
      "Epoch: 270, loss: 85.0308, score: 0.4175\n",
      "Epoch: 271, Loss: 77.8645\n",
      "Epoch: 272, Loss: 77.8322\n",
      "Epoch: 273, Loss: 77.8173\n",
      "Epoch: 274, Loss: 77.8162\n",
      "Epoch: 275, Loss: 77.7948\n",
      "Epoch: 276, Loss: 77.7663\n",
      "Epoch: 277, Loss: 77.7180\n",
      "Epoch: 278, Loss: 77.6999\n",
      "Epoch: 279, Loss: 77.7116\n",
      "Epoch: 280, Loss: 77.6820\n",
      "Epoch: 280, loss: 84.9120, score: 0.4183\n",
      "Epoch: 281, Loss: 77.6323\n",
      "Epoch: 282, Loss: 77.5978\n",
      "Epoch: 283, Loss: 77.5869\n",
      "Epoch: 284, Loss: 77.6268\n",
      "Epoch: 285, Loss: 77.6021\n",
      "Epoch: 286, Loss: 77.5311\n",
      "Epoch: 287, Loss: 77.4916\n",
      "Epoch: 288, Loss: 77.4896\n",
      "Epoch: 289, Loss: 77.5119\n",
      "Epoch: 290, Loss: 77.4552\n",
      "Epoch: 290, loss: 84.7801, score: 0.4192\n",
      "Epoch: 291, Loss: 77.4184\n",
      "Epoch: 292, Loss: 77.3705\n",
      "Epoch: 293, Loss: 77.3656\n",
      "Epoch: 294, Loss: 77.3574\n",
      "Epoch: 295, Loss: 77.3224\n",
      "Epoch: 296, Loss: 77.2832\n",
      "Epoch: 297, Loss: 77.2608\n",
      "Epoch: 298, Loss: 77.2579\n",
      "Epoch: 299, Loss: 77.2575\n",
      "Epoch: 300, Loss: 77.2183\n",
      "Epoch: 300, loss: 84.6965, score: 0.4198\n",
      "Epoch: 301, Loss: 77.1722\n",
      "Epoch: 302, Loss: 77.1651\n",
      "Epoch: 303, Loss: 77.1610\n",
      "Epoch: 304, Loss: 77.1540\n",
      "Epoch: 305, Loss: 77.1080\n",
      "Epoch: 306, Loss: 77.0685\n",
      "Epoch: 307, Loss: 77.0419\n",
      "Epoch: 308, Loss: 77.0414\n",
      "Epoch: 309, Loss: 77.0375\n",
      "Epoch: 310, Loss: 77.0126\n",
      "Epoch: 310, loss: 84.6345, score: 0.4202\n",
      "Epoch: 311, Loss: 77.0325\n",
      "Epoch: 312, Loss: 77.0025\n",
      "Epoch: 313, Loss: 76.9611\n",
      "Epoch: 314, Loss: 76.8974\n",
      "Epoch: 315, Loss: 76.8755\n",
      "Epoch: 316, Loss: 76.8872\n",
      "Epoch: 317, Loss: 76.8612\n",
      "Epoch: 318, Loss: 76.8308\n",
      "Epoch: 319, Loss: 76.7890\n",
      "Epoch: 320, Loss: 76.7774\n",
      "Epoch: 320, loss: 84.5484, score: 0.4208\n",
      "Epoch: 321, Loss: 76.7803\n",
      "Epoch: 322, Loss: 76.7609\n",
      "Epoch: 323, Loss: 76.7129\n",
      "Epoch: 324, Loss: 76.6883\n",
      "Epoch: 325, Loss: 76.6681\n",
      "Epoch: 326, Loss: 76.6467\n",
      "Epoch: 327, Loss: 76.6233\n",
      "Epoch: 328, Loss: 76.6095\n",
      "Epoch: 329, Loss: 76.6255\n",
      "Epoch: 330, Loss: 76.6123\n",
      "Epoch: 330, loss: 84.5320, score: 0.4209\n",
      "Epoch: 331, Loss: 76.6580\n",
      "Epoch: 332, Loss: 76.6705\n",
      "Epoch: 333, Loss: 76.6797\n",
      "Epoch: 334, Loss: 76.5380\n",
      "Epoch: 335, Loss: 76.5144\n",
      "Epoch: 336, Loss: 76.6063\n",
      "Epoch: 337, Loss: 76.6011\n",
      "Epoch: 338, Loss: 76.4244\n",
      "Epoch: 339, Loss: 76.4831\n",
      "Epoch: 340, Loss: 76.5230\n",
      "Epoch: 340, loss: 84.5837, score: 0.4205\n",
      "Epoch: 341, Loss: 76.5686\n",
      "Epoch: 342, Loss: 76.3728\n",
      "Epoch: 343, Loss: 76.4466\n",
      "Epoch: 344, Loss: 76.5021\n",
      "Epoch: 345, Loss: 76.4283\n",
      "Epoch: 346, Loss: 76.3211\n",
      "Epoch: 347, Loss: 76.4683\n",
      "Epoch: 348, Loss: 76.2902\n",
      "Epoch: 349, Loss: 76.3223\n",
      "Epoch: 350, Loss: 76.2818\n",
      "Epoch: 350, loss: 84.4903, score: 0.4212\n",
      "Epoch: 351, Loss: 76.3348\n",
      "Epoch: 352, Loss: 76.2028\n",
      "Epoch: 353, Loss: 76.2445\n",
      "Epoch: 354, Loss: 76.2307\n",
      "Epoch: 355, Loss: 76.1679\n",
      "Epoch: 356, Loss: 76.1829\n",
      "Epoch: 357, Loss: 76.1635\n",
      "Epoch: 358, Loss: 76.1157\n",
      "Epoch: 359, Loss: 76.0957\n",
      "Epoch: 360, Loss: 76.1613\n",
      "Epoch: 360, loss: 84.5030, score: 0.4211\n",
      "Epoch: 361, Loss: 76.0729\n",
      "Epoch: 362, Loss: 76.0420\n",
      "Epoch: 363, Loss: 76.0223\n",
      "Epoch: 364, Loss: 76.0535\n",
      "Epoch: 365, Loss: 75.9926\n",
      "Epoch: 366, Loss: 75.9666\n",
      "Epoch: 367, Loss: 75.9730\n",
      "Epoch: 368, Loss: 76.0154\n",
      "Epoch: 369, Loss: 75.9460\n",
      "Epoch: 370, Loss: 75.9037\n",
      "Epoch: 370, loss: 84.6181, score: 0.4203\n",
      "Epoch: 371, Loss: 75.9095\n",
      "Epoch: 372, Loss: 75.9430\n",
      "Epoch: 373, Loss: 75.8980\n",
      "Epoch: 374, Loss: 75.8468\n",
      "Epoch: 375, Loss: 75.8090\n",
      "Epoch: 376, Loss: 75.8408\n",
      "Epoch: 377, Loss: 75.8614\n",
      "Epoch: 378, Loss: 75.8027\n",
      "Epoch: 379, Loss: 75.7637\n",
      "Epoch: 380, Loss: 75.7973\n",
      "Epoch: 380, loss: 84.7645, score: 0.4193\n",
      "Epoch: 381, Loss: 75.7720\n",
      "Epoch: 382, Loss: 75.7047\n",
      "Epoch: 383, Loss: 75.7157\n",
      "Epoch: 384, Loss: 75.7115\n",
      "Epoch: 385, Loss: 75.7433\n",
      "Epoch: 386, Loss: 75.6508\n",
      "Epoch: 387, Loss: 75.7026\n",
      "Epoch: 388, Loss: 75.7391\n",
      "Epoch: 389, Loss: 75.7198\n",
      "Epoch: 390, Loss: 75.5711\n",
      "Epoch: 390, loss: 84.5603, score: 0.4207\n",
      "Epoch: 391, Loss: 75.6274\n",
      "Epoch: 392, Loss: 75.5895\n",
      "Epoch: 393, Loss: 75.5707\n",
      "Epoch: 394, Loss: 75.5086\n",
      "Epoch: 395, Loss: 75.5240\n",
      "Epoch: 396, Loss: 75.5217\n",
      "Epoch: 397, Loss: 75.4787\n",
      "Epoch: 398, Loss: 75.4533\n",
      "Epoch: 399, Loss: 75.4389\n",
      "Epoch: 400, Loss: 75.4634\n",
      "Epoch: 400, loss: 84.5586, score: 0.4207\n",
      "Epoch: 401, Loss: 75.4428\n",
      "Epoch: 402, Loss: 75.4288\n",
      "Epoch: 403, Loss: 75.4221\n",
      "Epoch: 404, Loss: 75.3870\n",
      "Epoch: 405, Loss: 75.3396\n",
      "Epoch: 406, Loss: 75.3069\n",
      "Epoch: 407, Loss: 75.3509\n",
      "Epoch: 408, Loss: 75.3995\n",
      "Epoch: 409, Loss: 75.3269\n",
      "Epoch: 410, Loss: 75.2957\n",
      "Epoch: 410, loss: 84.5867, score: 0.4205\n",
      "Epoch: 411, Loss: 75.2313\n",
      "Epoch: 412, Loss: 75.2243\n",
      "Epoch: 413, Loss: 75.2484\n",
      "Epoch: 414, Loss: 75.2383\n",
      "Epoch: 415, Loss: 75.2049\n",
      "Epoch: 416, Loss: 75.1843\n",
      "Epoch: 417, Loss: 75.1628\n",
      "Epoch: 418, Loss: 75.1466\n",
      "Epoch: 419, Loss: 75.1457\n",
      "Epoch: 420, Loss: 75.1352\n",
      "Epoch: 420, loss: 84.5029, score: 0.4211\n",
      "Epoch: 421, Loss: 75.1145\n",
      "Epoch: 422, Loss: 75.0656\n",
      "Epoch: 423, Loss: 75.0365\n",
      "Epoch: 424, Loss: 75.0190\n",
      "Epoch: 425, Loss: 74.9995\n",
      "Epoch: 426, Loss: 74.9856\n",
      "Epoch: 427, Loss: 74.9667\n",
      "Epoch: 428, Loss: 74.9538\n",
      "Epoch: 429, Loss: 74.9409\n",
      "Epoch: 430, Loss: 74.9444\n",
      "Epoch: 430, loss: 84.5768, score: 0.4206\n",
      "Epoch: 431, Loss: 74.9930\n",
      "Epoch: 432, Loss: 75.1035\n",
      "Epoch: 433, Loss: 74.9553\n",
      "Epoch: 434, Loss: 74.8806\n",
      "Epoch: 435, Loss: 74.8733\n",
      "Epoch: 436, Loss: 74.9162\n",
      "Epoch: 437, Loss: 74.9057\n",
      "Epoch: 438, Loss: 74.9865\n",
      "Epoch: 439, Loss: 74.7839\n",
      "Epoch: 440, Loss: 74.9292\n",
      "Epoch: 440, loss: 84.6299, score: 0.4202\n",
      "Epoch: 441, Loss: 74.8832\n",
      "Epoch: 442, Loss: 74.8953\n",
      "Epoch: 443, Loss: 74.8009\n",
      "Epoch: 444, Loss: 74.7688\n",
      "Epoch: 445, Loss: 74.7067\n",
      "Epoch: 446, Loss: 74.7460\n",
      "Epoch: 447, Loss: 74.8088\n",
      "Epoch: 448, Loss: 74.6888\n",
      "Epoch: 449, Loss: 74.6945\n",
      "Epoch: 450, Loss: 74.6755\n",
      "Epoch: 450, loss: 84.7674, score: 0.4193\n",
      "Epoch: 451, Loss: 74.7714\n",
      "Epoch: 452, Loss: 74.7450\n",
      "Epoch: 453, Loss: 74.6353\n",
      "Epoch: 454, Loss: 74.6207\n",
      "Epoch: 455, Loss: 74.5531\n",
      "Epoch: 456, Loss: 74.6159\n",
      "Epoch: 457, Loss: 74.6147\n",
      "Epoch: 458, Loss: 74.5405\n",
      "Epoch: 459, Loss: 74.5296\n",
      "Epoch: 460, Loss: 74.4774\n",
      "Epoch: 460, loss: 84.7009, score: 0.4197\n",
      "Epoch: 461, Loss: 74.5130\n",
      "Epoch: 462, Loss: 74.4897\n",
      "Epoch: 463, Loss: 74.4291\n",
      "Epoch: 464, Loss: 74.4401\n",
      "Epoch: 465, Loss: 74.4109\n",
      "Epoch: 466, Loss: 74.4258\n",
      "Epoch: 467, Loss: 74.4193\n",
      "Epoch: 468, Loss: 74.3734\n",
      "Epoch: 469, Loss: 74.3625\n",
      "Epoch: 470, Loss: 74.3225\n",
      "Epoch: 470, loss: 84.6053, score: 0.4204\n",
      "Epoch: 471, Loss: 74.3140\n",
      "Epoch: 472, Loss: 74.3159\n",
      "Epoch: 473, Loss: 74.2908\n",
      "Epoch: 474, Loss: 74.3137\n",
      "Epoch: 475, Loss: 74.3073\n",
      "Epoch: 476, Loss: 74.3035\n",
      "Epoch: 477, Loss: 74.2948\n",
      "Epoch: 478, Loss: 74.2471\n",
      "Epoch: 479, Loss: 74.1988\n",
      "Epoch: 480, Loss: 74.1855\n",
      "Epoch: 480, loss: 84.5724, score: 0.4206\n",
      "Epoch: 481, Loss: 74.1550\n",
      "Epoch: 482, Loss: 74.1638\n",
      "Epoch: 483, Loss: 74.1989\n",
      "Epoch: 484, Loss: 74.2026\n",
      "Epoch: 485, Loss: 74.2435\n",
      "Epoch: 486, Loss: 74.2180\n",
      "Epoch: 487, Loss: 74.1314\n",
      "Epoch: 488, Loss: 74.0544\n",
      "Epoch: 489, Loss: 74.0527\n",
      "Epoch: 490, Loss: 74.0811\n",
      "Epoch: 490, loss: 84.6627, score: 0.4200\n",
      "Epoch: 491, Loss: 74.0788\n",
      "Epoch: 492, Loss: 74.0621\n",
      "Epoch: 493, Loss: 74.0129\n",
      "Epoch: 494, Loss: 73.9715\n",
      "Epoch: 495, Loss: 73.9426\n",
      "Epoch: 496, Loss: 73.9309\n",
      "Epoch: 497, Loss: 73.9315\n",
      "Epoch: 498, Loss: 73.9363\n",
      "Epoch: 499, Loss: 73.9711\n",
      "Epoch: 500, Loss: 74.0070\n",
      "Epoch: 500, loss: 84.4965, score: 0.4211\n",
      "Epoch: 501, Loss: 73.9935\n",
      "Epoch: 502, Loss: 73.9413\n",
      "Epoch: 503, Loss: 73.8658\n",
      "Epoch: 504, Loss: 73.8083\n",
      "Epoch: 505, Loss: 73.8210\n",
      "Epoch: 506, Loss: 73.8477\n",
      "Epoch: 507, Loss: 73.8393\n",
      "Epoch: 508, Loss: 73.8021\n",
      "Epoch: 509, Loss: 73.7484\n",
      "Epoch: 510, Loss: 73.7189\n",
      "Epoch: 510, loss: 84.5857, score: 0.4205\n",
      "Epoch: 511, Loss: 73.7030\n",
      "Epoch: 512, Loss: 73.6937\n",
      "Epoch: 513, Loss: 73.6937\n",
      "Epoch: 514, Loss: 73.6884\n",
      "Epoch: 515, Loss: 73.7040\n",
      "Epoch: 516, Loss: 73.7259\n",
      "Epoch: 517, Loss: 73.7651\n",
      "Epoch: 518, Loss: 73.8072\n",
      "Epoch: 519, Loss: 73.7853\n",
      "Epoch: 520, Loss: 73.6688\n",
      "Epoch: 520, loss: 84.5660, score: 0.4207\n",
      "Epoch: 521, Loss: 73.5672\n",
      "Epoch: 522, Loss: 73.5653\n",
      "Epoch: 523, Loss: 73.6541\n",
      "Epoch: 524, Loss: 73.7810\n",
      "Epoch: 525, Loss: 73.8257\n",
      "Epoch: 526, Loss: 73.6144\n",
      "Epoch: 527, Loss: 73.4831\n",
      "Epoch: 528, Loss: 73.4966\n",
      "Epoch: 529, Loss: 73.5535\n",
      "Epoch: 530, Loss: 73.5560\n",
      "Epoch: 530, loss: 84.5524, score: 0.4207\n",
      "Epoch: 531, Loss: 73.5143\n",
      "Epoch: 532, Loss: 73.4592\n",
      "Epoch: 533, Loss: 73.3956\n",
      "Epoch: 534, Loss: 73.3776\n",
      "Epoch: 535, Loss: 73.4079\n",
      "Epoch: 536, Loss: 73.4025\n",
      "Epoch: 537, Loss: 73.3746\n",
      "Epoch: 538, Loss: 73.3447\n",
      "Epoch: 539, Loss: 73.3277\n",
      "Epoch: 540, Loss: 73.3283\n",
      "Epoch: 540, loss: 84.6369, score: 0.4202\n",
      "Epoch: 541, Loss: 73.3899\n",
      "Epoch: 542, Loss: 73.4605\n",
      "Epoch: 543, Loss: 73.5183\n",
      "Epoch: 544, Loss: 73.4096\n",
      "Epoch: 545, Loss: 73.2241\n",
      "Epoch: 546, Loss: 73.2497\n",
      "Epoch: 547, Loss: 73.3559\n",
      "Epoch: 548, Loss: 73.3686\n",
      "Epoch: 549, Loss: 73.2948\n",
      "Epoch: 550, Loss: 73.1579\n",
      "Epoch: 550, loss: 84.6476, score: 0.4201\n",
      "Epoch: 551, Loss: 73.1797\n",
      "Epoch: 552, Loss: 73.3321\n",
      "Epoch: 553, Loss: 73.4400\n",
      "Epoch: 554, Loss: 73.3055\n",
      "Epoch: 555, Loss: 73.0958\n",
      "Epoch: 556, Loss: 73.0935\n",
      "Epoch: 557, Loss: 73.2550\n",
      "Epoch: 558, Loss: 73.3511\n",
      "Epoch: 559, Loss: 73.1663\n",
      "Epoch: 560, Loss: 72.9988\n",
      "Epoch: 560, loss: 84.5953, score: 0.4205\n",
      "Epoch: 561, Loss: 73.1440\n",
      "Epoch: 562, Loss: 73.2302\n",
      "Epoch: 563, Loss: 73.0700\n",
      "Epoch: 564, Loss: 73.0092\n",
      "Epoch: 565, Loss: 73.0675\n",
      "Epoch: 566, Loss: 73.1204\n",
      "Epoch: 567, Loss: 73.0261\n",
      "Epoch: 568, Loss: 72.9039\n",
      "Epoch: 569, Loss: 72.9888\n",
      "Epoch: 570, Loss: 72.9387\n",
      "Epoch: 570, loss: 84.6551, score: 0.4200\n",
      "Epoch: 571, Loss: 72.9914\n",
      "Epoch: 572, Loss: 73.0232\n",
      "Epoch: 573, Loss: 72.8196\n",
      "Epoch: 574, Loss: 73.0586\n",
      "Epoch: 575, Loss: 72.9474\n",
      "Epoch: 576, Loss: 72.8837\n",
      "Epoch: 577, Loss: 72.9203\n",
      "Epoch: 578, Loss: 72.8057\n",
      "Epoch: 579, Loss: 72.9252\n",
      "Epoch: 580, Loss: 72.8313\n",
      "Epoch: 580, loss: 84.6465, score: 0.4201\n",
      "Epoch: 581, Loss: 72.7126\n",
      "Epoch: 582, Loss: 72.8382\n",
      "Epoch: 583, Loss: 72.7987\n",
      "Epoch: 584, Loss: 72.7787\n",
      "Epoch: 585, Loss: 72.8359\n",
      "Epoch: 586, Loss: 72.6447\n",
      "Epoch: 587, Loss: 72.8668\n",
      "Epoch: 588, Loss: 72.7652\n",
      "Epoch: 589, Loss: 72.6500\n",
      "Epoch: 590, Loss: 72.7770\n",
      "Epoch: 590, loss: 84.6590, score: 0.4200\n",
      "Epoch: 591, Loss: 72.6761\n",
      "Epoch: 592, Loss: 72.7238\n",
      "Epoch: 593, Loss: 72.6960\n",
      "Epoch: 594, Loss: 72.5483\n",
      "Epoch: 595, Loss: 72.7075\n",
      "Epoch: 596, Loss: 72.6890\n",
      "Epoch: 597, Loss: 72.4985\n",
      "Epoch: 598, Loss: 72.6497\n",
      "Epoch: 599, Loss: 72.5791\n",
      "Epoch: 600, Loss: 72.4579\n",
      "Epoch: 600, loss: 84.6613, score: 0.4200\n",
      "Epoch: 601, Loss: 72.6200\n",
      "Epoch: 602, Loss: 72.4955\n",
      "Epoch: 603, Loss: 72.4132\n",
      "Epoch: 604, Loss: 72.4955\n",
      "Epoch: 605, Loss: 72.4081\n",
      "Epoch: 606, Loss: 72.3989\n",
      "Epoch: 607, Loss: 72.4471\n",
      "Epoch: 608, Loss: 72.3668\n",
      "Epoch: 609, Loss: 72.3396\n",
      "Epoch: 610, Loss: 72.3655\n",
      "Epoch: 610, loss: 84.6695, score: 0.4199\n",
      "Epoch: 611, Loss: 72.3020\n",
      "Epoch: 612, Loss: 72.3279\n",
      "Epoch: 613, Loss: 72.3758\n",
      "Epoch: 614, Loss: 72.3126\n",
      "Epoch: 615, Loss: 72.3292\n",
      "Epoch: 616, Loss: 72.3108\n",
      "Epoch: 617, Loss: 72.2521\n",
      "Epoch: 618, Loss: 72.2194\n",
      "Epoch: 619, Loss: 72.2733\n",
      "Epoch: 620, Loss: 72.2717\n",
      "Epoch: 620, loss: 84.7016, score: 0.4197\n",
      "Epoch: 621, Loss: 72.1626\n",
      "Epoch: 622, Loss: 72.1774\n",
      "Epoch: 623, Loss: 72.2102\n",
      "Epoch: 624, Loss: 72.1699\n",
      "Epoch: 625, Loss: 72.2327\n",
      "Epoch: 626, Loss: 72.2594\n",
      "Epoch: 627, Loss: 72.1782\n",
      "Epoch: 628, Loss: 72.1224\n",
      "Epoch: 629, Loss: 72.1329\n",
      "Epoch: 630, Loss: 72.0793\n",
      "Epoch: 630, loss: 84.7798, score: 0.4192\n",
      "Epoch: 631, Loss: 72.0824\n",
      "Epoch: 632, Loss: 72.1571\n",
      "Epoch: 633, Loss: 72.1295\n",
      "Epoch: 634, Loss: 72.2325\n",
      "Epoch: 635, Loss: 72.0164\n",
      "Epoch: 636, Loss: 71.9583\n",
      "Epoch: 637, Loss: 71.9596\n",
      "Epoch: 638, Loss: 71.9438\n",
      "Epoch: 639, Loss: 71.9439\n",
      "Epoch: 640, Loss: 71.9254\n",
      "Epoch: 640, loss: 84.7510, score: 0.4194\n",
      "Epoch: 641, Loss: 71.9211\n",
      "Epoch: 642, Loss: 71.9017\n",
      "Epoch: 643, Loss: 71.8967\n",
      "Epoch: 644, Loss: 71.8773\n",
      "Epoch: 645, Loss: 71.8799\n",
      "Epoch: 646, Loss: 71.8722\n",
      "Epoch: 647, Loss: 71.9164\n",
      "Epoch: 648, Loss: 71.9302\n",
      "Epoch: 649, Loss: 72.1198\n",
      "Epoch: 650, Loss: 71.9145\n",
      "Epoch: 650, loss: 84.9255, score: 0.4182\n",
      "Epoch: 651, Loss: 71.8581\n",
      "Epoch: 652, Loss: 71.7821\n",
      "Epoch: 653, Loss: 71.7581\n",
      "Epoch: 654, Loss: 71.8203\n",
      "Epoch: 655, Loss: 71.8204\n",
      "Epoch: 656, Loss: 71.8240\n",
      "Epoch: 657, Loss: 71.8758\n",
      "Epoch: 658, Loss: 72.0257\n",
      "Epoch: 659, Loss: 71.7121\n",
      "Epoch: 660, Loss: 72.0077\n",
      "Epoch: 660, loss: 84.7692, score: 0.4193\n",
      "Epoch: 661, Loss: 71.9375\n",
      "Epoch: 662, Loss: 71.9430\n",
      "Epoch: 663, Loss: 72.3540\n",
      "Epoch: 664, Loss: 71.6233\n",
      "Epoch: 665, Loss: 72.3181\n",
      "Epoch: 666, Loss: 72.0761\n",
      "Epoch: 667, Loss: 72.9122\n",
      "Epoch: 668, Loss: 71.9308\n",
      "Epoch: 669, Loss: 72.2149\n",
      "Epoch: 670, Loss: 71.8544\n",
      "Epoch: 670, loss: 85.0701, score: 0.4172\n",
      "Epoch: 671, Loss: 72.2650\n",
      "Epoch: 672, Loss: 71.9162\n",
      "Epoch: 673, Loss: 72.2665\n",
      "Epoch: 674, Loss: 72.4121\n",
      "Epoch: 675, Loss: 71.5582\n",
      "Epoch: 676, Loss: 72.4551\n",
      "Epoch: 677, Loss: 71.8460\n",
      "Epoch: 678, Loss: 72.2385\n",
      "Epoch: 679, Loss: 71.9096\n",
      "Epoch: 680, Loss: 71.6206\n",
      "Epoch: 680, loss: 84.9842, score: 0.4178\n",
      "Epoch: 681, Loss: 71.9101\n",
      "Epoch: 682, Loss: 71.7726\n",
      "Epoch: 683, Loss: 71.8671\n",
      "Epoch: 684, Loss: 71.4383\n",
      "Epoch: 685, Loss: 71.8896\n",
      "Epoch: 686, Loss: 71.3990\n",
      "Epoch: 687, Loss: 71.7856\n",
      "Epoch: 688, Loss: 71.5135\n",
      "Epoch: 689, Loss: 71.9007\n",
      "Epoch: 690, Loss: 71.4172\n",
      "Epoch: 690, loss: 84.6146, score: 0.4203\n",
      "Epoch: 691, Loss: 71.6394\n",
      "Epoch: 692, Loss: 71.5351\n",
      "Epoch: 693, Loss: 71.4951\n",
      "Epoch: 694, Loss: 71.3576\n",
      "Epoch: 695, Loss: 71.4837\n",
      "Epoch: 696, Loss: 71.4148\n",
      "Epoch: 697, Loss: 71.3824\n",
      "Epoch: 698, Loss: 71.2758\n",
      "Epoch: 699, Loss: 71.2962\n",
      "Epoch: 700, Loss: 71.2181\n",
      "Epoch: 700, loss: 84.7308, score: 0.4195\n",
      "Epoch: 701, Loss: 71.2422\n",
      "Epoch: 702, Loss: 71.1936\n",
      "Epoch: 703, Loss: 71.2103\n",
      "Epoch: 704, Loss: 71.1594\n",
      "Epoch: 705, Loss: 71.1784\n",
      "Epoch: 706, Loss: 71.1390\n",
      "Epoch: 707, Loss: 71.1388\n",
      "Epoch: 708, Loss: 71.1312\n",
      "Epoch: 709, Loss: 71.0883\n",
      "Epoch: 710, Loss: 71.1070\n",
      "Epoch: 710, loss: 84.8185, score: 0.4189\n",
      "Epoch: 711, Loss: 71.0768\n",
      "Epoch: 712, Loss: 71.0721\n",
      "Epoch: 713, Loss: 71.0597\n",
      "Epoch: 714, Loss: 71.0193\n",
      "Epoch: 715, Loss: 71.0181\n",
      "Epoch: 716, Loss: 71.0080\n",
      "Epoch: 717, Loss: 70.9906\n",
      "Epoch: 718, Loss: 70.9952\n",
      "Epoch: 719, Loss: 70.9641\n",
      "Epoch: 720, Loss: 70.9432\n",
      "Epoch: 720, loss: 84.8891, score: 0.4184\n",
      "Epoch: 721, Loss: 70.9414\n",
      "Epoch: 722, Loss: 70.9199\n",
      "Epoch: 723, Loss: 70.8998\n",
      "Epoch: 724, Loss: 70.8898\n",
      "Epoch: 725, Loss: 70.8702\n",
      "Epoch: 726, Loss: 70.8599\n",
      "Epoch: 727, Loss: 70.8437\n",
      "Epoch: 728, Loss: 70.8311\n",
      "Epoch: 729, Loss: 70.8195\n",
      "Epoch: 730, Loss: 70.8023\n",
      "Epoch: 730, loss: 84.8605, score: 0.4186\n",
      "Epoch: 731, Loss: 70.7889\n",
      "Epoch: 732, Loss: 70.7805\n",
      "Epoch: 733, Loss: 70.7831\n",
      "Epoch: 734, Loss: 70.7837\n",
      "Epoch: 735, Loss: 70.8047\n",
      "Epoch: 736, Loss: 70.9275\n",
      "Epoch: 737, Loss: 71.0016\n",
      "Epoch: 738, Loss: 71.1698\n",
      "Epoch: 739, Loss: 71.1802\n",
      "Epoch: 740, Loss: 71.2375\n",
      "Epoch: 740, loss: 84.8940, score: 0.4184\n",
      "Epoch: 741, Loss: 70.7761\n",
      "Epoch: 742, Loss: 70.8590\n",
      "Epoch: 743, Loss: 71.1815\n",
      "Epoch: 744, Loss: 71.3370\n",
      "Epoch: 745, Loss: 71.3279\n",
      "Epoch: 746, Loss: 70.7215\n",
      "Epoch: 747, Loss: 71.4706\n",
      "Epoch: 748, Loss: 71.4744\n",
      "Epoch: 749, Loss: 71.1721\n",
      "Epoch: 750, Loss: 70.7694\n",
      "Epoch: 750, loss: 84.8021, score: 0.4190\n",
      "Epoch: 751, Loss: 70.7036\n",
      "Epoch: 752, Loss: 70.6736\n",
      "Epoch: 753, Loss: 70.7820\n",
      "Epoch: 754, Loss: 70.6236\n",
      "Epoch: 755, Loss: 70.7255\n",
      "Epoch: 756, Loss: 70.5882\n",
      "Epoch: 757, Loss: 70.7122\n",
      "Epoch: 758, Loss: 70.7915\n",
      "Epoch: 759, Loss: 70.6489\n",
      "Epoch: 760, Loss: 70.8412\n",
      "Epoch: 760, loss: 84.9941, score: 0.4177\n",
      "Epoch: 761, Loss: 70.7275\n",
      "Epoch: 762, Loss: 70.8564\n",
      "Epoch: 763, Loss: 70.5899\n",
      "Epoch: 764, Loss: 70.4665\n",
      "Epoch: 765, Loss: 70.6786\n",
      "Epoch: 766, Loss: 70.6209\n",
      "Epoch: 767, Loss: 70.7042\n",
      "Epoch: 768, Loss: 70.5722\n",
      "Epoch: 769, Loss: 70.5676\n",
      "Epoch: 770, Loss: 70.4398\n",
      "Epoch: 770, loss: 84.9169, score: 0.4183\n",
      "Epoch: 771, Loss: 70.3993\n",
      "Epoch: 772, Loss: 70.4102\n",
      "Epoch: 773, Loss: 70.3127\n",
      "Epoch: 774, Loss: 70.4385\n",
      "Epoch: 775, Loss: 70.3745\n",
      "Epoch: 776, Loss: 70.3159\n",
      "Epoch: 777, Loss: 70.4212\n",
      "Epoch: 778, Loss: 70.2769\n",
      "Epoch: 779, Loss: 70.3002\n",
      "Epoch: 780, Loss: 70.3374\n",
      "Epoch: 780, loss: 84.8696, score: 0.4186\n",
      "Epoch: 781, Loss: 70.2109\n",
      "Epoch: 782, Loss: 70.2626\n",
      "Epoch: 783, Loss: 70.1924\n",
      "Epoch: 784, Loss: 70.1831\n",
      "Epoch: 785, Loss: 70.1715\n",
      "Epoch: 786, Loss: 70.1296\n",
      "Epoch: 787, Loss: 70.1325\n",
      "Epoch: 788, Loss: 70.1170\n",
      "Epoch: 789, Loss: 70.1184\n",
      "Epoch: 790, Loss: 70.1157\n",
      "Epoch: 790, loss: 84.9616, score: 0.4179\n",
      "Epoch: 791, Loss: 70.1000\n",
      "Epoch: 792, Loss: 70.0938\n",
      "Epoch: 793, Loss: 70.0748\n",
      "Epoch: 794, Loss: 70.0393\n",
      "Epoch: 795, Loss: 70.0148\n",
      "Epoch: 796, Loss: 69.9987\n",
      "Epoch: 797, Loss: 70.0048\n",
      "Epoch: 798, Loss: 69.9973\n",
      "Epoch: 799, Loss: 69.9919\n",
      "Epoch: 800, Loss: 69.9929\n",
      "Epoch: 800, loss: 85.1079, score: 0.4169\n",
      "Epoch: 801, Loss: 69.9986\n",
      "Epoch: 802, Loss: 70.0248\n",
      "Epoch: 803, Loss: 70.0443\n",
      "Epoch: 804, Loss: 70.1628\n",
      "Epoch: 805, Loss: 70.0284\n",
      "Epoch: 806, Loss: 69.9920\n",
      "Epoch: 807, Loss: 69.9386\n",
      "Epoch: 808, Loss: 69.8766\n",
      "Epoch: 809, Loss: 69.8565\n",
      "Epoch: 810, Loss: 69.8816\n",
      "Epoch: 810, loss: 85.0636, score: 0.4172\n",
      "Epoch: 811, Loss: 69.8034\n",
      "Epoch: 812, Loss: 69.8054\n",
      "Epoch: 813, Loss: 69.8564\n",
      "Epoch: 814, Loss: 69.7986\n",
      "Epoch: 815, Loss: 69.7929\n",
      "Epoch: 816, Loss: 69.8112\n",
      "Epoch: 817, Loss: 69.8455\n",
      "Epoch: 818, Loss: 69.8935\n",
      "Epoch: 819, Loss: 70.0539\n",
      "Epoch: 820, Loss: 69.8492\n",
      "Epoch: 820, loss: 85.3469, score: 0.4153\n",
      "Epoch: 821, Loss: 69.9106\n",
      "Epoch: 822, Loss: 69.9178\n",
      "Epoch: 823, Loss: 69.6567\n",
      "Epoch: 824, Loss: 69.8235\n",
      "Epoch: 825, Loss: 69.9718\n",
      "Epoch: 826, Loss: 69.6837\n",
      "Epoch: 827, Loss: 70.1504\n",
      "Epoch: 828, Loss: 70.0717\n",
      "Epoch: 829, Loss: 70.1453\n",
      "Epoch: 830, Loss: 70.0911\n",
      "Epoch: 830, loss: 85.2340, score: 0.4161\n",
      "Epoch: 831, Loss: 69.6948\n",
      "Epoch: 832, Loss: 69.9792\n",
      "Epoch: 833, Loss: 69.6776\n",
      "Epoch: 834, Loss: 70.1903\n",
      "Epoch: 835, Loss: 70.0593\n",
      "Epoch: 836, Loss: 70.3017\n",
      "Epoch: 837, Loss: 69.8564\n",
      "Epoch: 838, Loss: 69.8262\n",
      "Epoch: 839, Loss: 69.8861\n",
      "Epoch: 840, Loss: 69.7677\n",
      "Epoch: 840, loss: 85.3895, score: 0.4150\n",
      "Epoch: 841, Loss: 69.8492\n",
      "Epoch: 842, Loss: 69.6311\n",
      "Epoch: 843, Loss: 69.6512\n",
      "Epoch: 844, Loss: 69.4319\n",
      "Epoch: 845, Loss: 69.6848\n",
      "Epoch: 846, Loss: 69.4718\n",
      "Epoch: 847, Loss: 69.4899\n",
      "Epoch: 848, Loss: 69.4861\n",
      "Epoch: 849, Loss: 69.3771\n",
      "Epoch: 850, Loss: 69.3907\n",
      "Epoch: 850, loss: 85.0518, score: 0.4173\n",
      "Epoch: 851, Loss: 69.3278\n",
      "Epoch: 852, Loss: 69.4426\n",
      "Epoch: 853, Loss: 69.3559\n",
      "Epoch: 854, Loss: 69.3847\n",
      "Epoch: 855, Loss: 69.3849\n",
      "Epoch: 856, Loss: 69.2920\n",
      "Epoch: 857, Loss: 69.2972\n",
      "Epoch: 858, Loss: 69.2511\n",
      "Epoch: 859, Loss: 69.2079\n",
      "Epoch: 860, Loss: 69.2144\n",
      "Epoch: 860, loss: 85.1822, score: 0.4164\n",
      "Epoch: 861, Loss: 69.2037\n",
      "Epoch: 862, Loss: 69.1924\n",
      "Epoch: 863, Loss: 69.1987\n",
      "Epoch: 864, Loss: 69.1485\n",
      "Epoch: 865, Loss: 69.1233\n",
      "Epoch: 866, Loss: 69.1563\n",
      "Epoch: 867, Loss: 69.0957\n",
      "Epoch: 868, Loss: 69.0978\n",
      "Epoch: 869, Loss: 69.1599\n",
      "Epoch: 870, Loss: 69.0535\n",
      "Epoch: 870, loss: 85.4020, score: 0.4149\n",
      "Epoch: 871, Loss: 69.1611\n",
      "Epoch: 872, Loss: 69.2092\n",
      "Epoch: 873, Loss: 69.0137\n",
      "Epoch: 874, Loss: 69.1283\n",
      "Epoch: 875, Loss: 69.1906\n",
      "Epoch: 876, Loss: 68.9702\n",
      "Epoch: 877, Loss: 69.0764\n",
      "Epoch: 878, Loss: 69.1410\n",
      "Epoch: 879, Loss: 68.9358\n",
      "Epoch: 880, Loss: 69.0340\n",
      "Epoch: 880, loss: 85.1853, score: 0.4164\n",
      "Epoch: 881, Loss: 69.1132\n",
      "Epoch: 882, Loss: 68.9028\n",
      "Epoch: 883, Loss: 69.1304\n",
      "Epoch: 884, Loss: 69.2772\n",
      "Epoch: 885, Loss: 69.3061\n",
      "Epoch: 886, Loss: 69.7950\n",
      "Epoch: 887, Loss: 69.1112\n",
      "Epoch: 888, Loss: 69.7551\n",
      "Epoch: 889, Loss: 69.4385\n",
      "Epoch: 890, Loss: 69.4711\n",
      "Epoch: 890, loss: 85.5208, score: 0.4141\n",
      "Epoch: 891, Loss: 69.1034\n",
      "Epoch: 892, Loss: 69.7245\n",
      "Epoch: 893, Loss: 69.7464\n",
      "Epoch: 894, Loss: 70.1148\n",
      "Epoch: 895, Loss: 69.8309\n",
      "Epoch: 896, Loss: 69.4832\n",
      "Epoch: 897, Loss: 68.7991\n",
      "Epoch: 898, Loss: 69.1293\n",
      "Epoch: 899, Loss: 69.0525\n",
      "Epoch: 900, Loss: 69.1955\n",
      "Epoch: 900, loss: 85.3599, score: 0.4152\n",
      "Epoch: 901, Loss: 68.7663\n",
      "Epoch: 902, Loss: 68.9164\n",
      "Epoch: 903, Loss: 69.0119\n",
      "Epoch: 904, Loss: 69.1976\n",
      "Epoch: 905, Loss: 69.0195\n",
      "Epoch: 906, Loss: 68.7414\n",
      "Epoch: 907, Loss: 68.9409\n",
      "Epoch: 908, Loss: 68.8986\n",
      "Epoch: 909, Loss: 68.9402\n",
      "Epoch: 910, Loss: 68.6644\n",
      "Epoch: 910, loss: 85.2219, score: 0.4162\n",
      "Epoch: 911, Loss: 68.6915\n",
      "Epoch: 912, Loss: 68.6375\n",
      "Epoch: 913, Loss: 68.7234\n",
      "Epoch: 914, Loss: 68.6728\n",
      "Epoch: 915, Loss: 68.5661\n",
      "Epoch: 916, Loss: 68.5650\n",
      "Epoch: 917, Loss: 68.5551\n",
      "Epoch: 918, Loss: 68.6231\n",
      "Epoch: 919, Loss: 68.4970\n",
      "Epoch: 920, Loss: 68.5572\n",
      "Epoch: 920, loss: 85.3642, score: 0.4152\n",
      "Epoch: 921, Loss: 68.4828\n",
      "Epoch: 922, Loss: 68.6107\n",
      "Epoch: 923, Loss: 68.5977\n",
      "Epoch: 924, Loss: 68.4400\n",
      "Epoch: 925, Loss: 68.5603\n",
      "Epoch: 926, Loss: 68.3673\n",
      "Epoch: 927, Loss: 68.7064\n",
      "Epoch: 928, Loss: 68.6618\n",
      "Epoch: 929, Loss: 68.6198\n",
      "Epoch: 930, Loss: 68.6369\n",
      "Epoch: 930, loss: 85.5462, score: 0.4139\n",
      "Epoch: 931, Loss: 68.3496\n",
      "Epoch: 932, Loss: 68.5248\n",
      "Epoch: 933, Loss: 68.3700\n",
      "Epoch: 934, Loss: 68.7653\n",
      "Epoch: 935, Loss: 68.6342\n",
      "Epoch: 936, Loss: 68.5819\n",
      "Epoch: 937, Loss: 68.4147\n",
      "Epoch: 938, Loss: 68.2354\n",
      "Epoch: 939, Loss: 68.4424\n",
      "Epoch: 940, Loss: 68.3656\n",
      "Epoch: 940, loss: 85.9293, score: 0.4113\n",
      "Epoch: 941, Loss: 68.5481\n",
      "Epoch: 942, Loss: 68.3955\n",
      "Epoch: 943, Loss: 68.2281\n",
      "Epoch: 944, Loss: 68.1941\n",
      "Epoch: 945, Loss: 68.1391\n",
      "Epoch: 946, Loss: 68.2252\n",
      "Epoch: 947, Loss: 68.2176\n",
      "Epoch: 948, Loss: 68.1744\n",
      "Epoch: 949, Loss: 68.0973\n",
      "Epoch: 950, Loss: 68.0547\n",
      "Epoch: 950, loss: 85.6031, score: 0.4135\n",
      "Epoch: 951, Loss: 68.0638\n",
      "Epoch: 952, Loss: 68.0843\n",
      "Epoch: 953, Loss: 68.0927\n",
      "Epoch: 954, Loss: 68.1200\n",
      "Epoch: 955, Loss: 68.0451\n",
      "Epoch: 956, Loss: 67.9753\n",
      "Epoch: 957, Loss: 67.9921\n",
      "Epoch: 958, Loss: 68.0381\n",
      "Epoch: 959, Loss: 67.9497\n",
      "Epoch: 960, Loss: 68.1355\n",
      "Epoch: 960, loss: 85.6145, score: 0.4135\n",
      "Epoch: 961, Loss: 67.9997\n",
      "Epoch: 962, Loss: 68.0753\n",
      "Epoch: 963, Loss: 68.0639\n",
      "Epoch: 964, Loss: 67.8975\n",
      "Epoch: 965, Loss: 67.9491\n",
      "Epoch: 966, Loss: 67.8502\n",
      "Epoch: 967, Loss: 67.9015\n",
      "Epoch: 968, Loss: 68.0315\n",
      "Epoch: 969, Loss: 67.8798\n",
      "Epoch: 970, Loss: 67.9579\n",
      "Epoch: 970, loss: 85.8272, score: 0.4120\n",
      "Epoch: 971, Loss: 68.0306\n",
      "Epoch: 972, Loss: 67.8665\n",
      "Epoch: 973, Loss: 68.0162\n",
      "Epoch: 974, Loss: 67.9115\n",
      "Epoch: 975, Loss: 67.8501\n",
      "Epoch: 976, Loss: 67.8800\n",
      "Epoch: 977, Loss: 67.7425\n",
      "Epoch: 978, Loss: 67.7507\n",
      "Epoch: 979, Loss: 67.6833\n",
      "Epoch: 980, Loss: 67.7577\n",
      "Epoch: 980, loss: 85.7413, score: 0.4126\n",
      "Epoch: 981, Loss: 67.7833\n",
      "Epoch: 982, Loss: 67.7546\n",
      "Epoch: 983, Loss: 67.9683\n",
      "Epoch: 984, Loss: 67.7937\n",
      "Epoch: 985, Loss: 67.7823\n",
      "Epoch: 986, Loss: 67.7294\n",
      "Epoch: 987, Loss: 67.6889\n",
      "Epoch: 988, Loss: 67.6698\n",
      "Epoch: 989, Loss: 67.6379\n",
      "Epoch: 990, Loss: 67.6027\n",
      "Epoch: 990, loss: 85.8130, score: 0.4121\n",
      "Epoch: 991, Loss: 67.5885\n",
      "Epoch: 992, Loss: 67.5587\n",
      "Epoch: 993, Loss: 67.5381\n",
      "Epoch: 994, Loss: 67.5192\n",
      "Epoch: 995, Loss: 67.5239\n",
      "Epoch: 996, Loss: 67.5185\n",
      "Epoch: 997, Loss: 67.5023\n",
      "Epoch: 998, Loss: 67.4708\n",
      "Epoch: 999, Loss: 67.4508\n",
      "Epoch: 1000, Loss: 67.4670\n",
      "Epoch: 1000, loss: 85.9076, score: 0.4115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test-loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test-score</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>train-score</td><td>▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>training-loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test-loss</td><td>85.90762</td></tr><tr><td>test-score</td><td>0.41146</td></tr><tr><td>train-score</td><td>0.50655</td></tr><tr><td>training-loss</td><td>67.467</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-breeze-15</strong> at: <a href='https://wandb.ai/damix48/movie-actor/runs/xkuwmfqz' target=\"_blank\">https://wandb.ai/damix48/movie-actor/runs/xkuwmfqz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240126_161406-xkuwmfqz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "        self.conv2 = GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "        self.lin1 = Linear(-1, hidden_channels)\n",
    "        self.lin2 = Linear(-1, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "data.to(device)\n",
    "\n",
    "model = GNN(hidden_channels=128, out_channels=1)\n",
    "model = to_hetero(model, data.metadata(), aggr=\"mean\").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "random_indices = torch.randperm(data[\"actor\"].num_nodes)\n",
    "train_indices = random_indices[: int(data[\"actor\"].num_nodes * 0.8)]\n",
    "test_indices = random_indices[int(data[\"actor\"].num_nodes * 0.8) :]\n",
    "\n",
    "wandb.init(project=\"movie-actor\")\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = F.mse_loss(out[\"actor\"][train_indices], data[\"actor\"].y[train_indices])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}\")\n",
    "    wandb.log({\"training-loss\": loss})\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "        y_pred = out[\"actor\"][train_indices].detach().cpu().numpy()\n",
    "        y_true = data[\"actor\"].y[train_indices].detach().cpu().numpy()\n",
    "        score = r2_score(y_true, y_pred)\n",
    "        wandb.log({\"train-score\": score})\n",
    "\n",
    "        y_pred = out[\"actor\"][test_indices].detach().cpu().numpy()\n",
    "        y_true = data[\"actor\"].y[test_indices].detach().cpu().numpy()\n",
    "        score = r2_score(y_true, y_pred)\n",
    "        wandb.log({\"test-score\": score})\n",
    "\n",
    "        test_loss = F.mse_loss(\n",
    "            out[\"actor\"][test_indices], data[\"actor\"].y[test_indices]\n",
    "        )\n",
    "        wandb.log({\"test-loss\": test_loss})\n",
    "\n",
    "        print(f\"Epoch: {epoch:03d}, loss: {test_loss:.4f}, score: {score:.4f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
